# memory.ts - Phase 2 Memory System Types

## Overview
`memory.ts` defines TypeScript interfaces for Kalito's Phase 2 memory management system. These types support conversation summarization, semantic pinning, importance scoring, and context window management for AI chat sessions.

**Date:** August 24, 2025  
**Phase:** 2 (Advanced memory features)

---

## Core Memory Types

### ConversationSummary (Lines 6-14)

```typescript
export interface ConversationSummary {
  id: string
  session_id: string
  summary: string
  message_count: number
  start_message_id: string
  end_message_id: string
  importance_score: number
  created_at: string
}
```

**Purpose:**
Represents a condensed summary of a conversation segment, reducing token usage while preserving context.

---

#### Why Summaries?

**Problem:** Long conversations exceed context windows

```
Session starts:
User: "My dad has diabetes"         [10 tokens]
AI: "I understand. What's his..."   [50 tokens]
User: "He takes Metformin..."       [20 tokens]
AI: "Metformin is an important..."  [100 tokens]
... 100 more messages ...
[Total: 50,000 tokens] ‚ùå Exceeds 32K context window!
```

**Solution:** Summarize old messages

```
Summary: "User's father has type 2 diabetes, takes Metformin 500mg twice daily, 
has no known allergies. Discussed medication adherence and dietary changes."
[100 tokens] ‚úÖ Saves 4,900 tokens!

Recent messages (full text):
User: "Should he check glucose before or after meals?"
AI: "Blood glucose should be checked..."
[Total: 100 (summary) + 500 (recent) = 600 tokens] ‚úÖ Fits easily!
```

---

#### Property: id
```typescript
id: string
```

**What it is:** Unique identifier for the summary

**Example:**
```typescript
id: "1698501234567-abc123xyz"  // Generated by generateId()
```

---

#### Property: session_id
```typescript
session_id: string
```

**What it is:** Which chat session this summary belongs to

**Example:**
```typescript
session_id: "session-123"
```

**Use case:** Query all summaries for a session
```sql
SELECT * FROM conversation_summaries WHERE session_id = 'session-123' ORDER BY created_at
```

---

#### Property: summary
```typescript
summary: string
```

**What it is:** The condensed summary text

**Example:**
```typescript
summary: "Patient John Doe, 75 years old, has type 2 diabetes managed with Metformin 500mg BID. No known drug allergies. Recent A1C: 7.2%. Discussed importance of daily glucose monitoring and foot care. Caregiver is daughter Jane, lives nearby, visits 3x/week."
```

**Generation method:**
- AI-generated (send old messages to LLM: "Summarize this conversation")
- Extractive (pull key facts from messages)
- Hybrid (AI summarize + extract entities)

---

#### Property: message_count
```typescript
message_count: number
```

**What it is:** How many messages were summarized

**Example:**
```typescript
message_count: 50  // This summary condenses 50 messages
```

**Use case:** Compression ratio calculation
```typescript
const compressionRatio = summary.length / (message_count * avgMessageLength)
// Lower ratio = better compression
```

---

#### Property: start_message_id & end_message_id
```typescript
start_message_id: string
end_message_id: string
```

**What it is:** Range of messages included in the summary

**Example:**
```typescript
start_message_id: "msg-100"  // First message in summary
end_message_id: "msg-150"    // Last message in summary
```

**Use cases:**
- **Traceability**: Can retrieve original messages if needed
- **Non-overlapping summaries**: Ensure each message is summarized once
- **Deletion safety**: If messages are deleted, know which summary is affected

**Example flow:**
```typescript
// Session has messages 1-200
// Create summary for messages 1-100
summary1: { start_message_id: "msg-1", end_message_id: "msg-100" }

// Later, create summary for messages 101-200
summary2: { start_message_id: "msg-101", end_message_id: "msg-200" }

// No gaps, no overlaps
```

---

#### Property: importance_score
```typescript
importance_score: number
```

**What it is:** Numerical rating of summary importance (0-10 scale)

**Example:**
```typescript
importance_score: 8.5  // High importance
importance_score: 3.0  // Low importance
```

**Scoring factors:**
- Medical keywords (high importance)
- Medication changes (high)
- Casual chat (low)
- Critical alerts (10.0)

**Use case:** Context window prioritization
```typescript
// When context window is full, keep high-importance summaries
const context = summaries
  .filter(s => s.importance_score >= 7)
  .sort((a, b) => b.importance_score - a.importance_score)
  .slice(0, 3)  // Keep top 3 most important
```

---

#### Property: created_at
```typescript
created_at: string
```

**What it is:** ISO timestamp of summary creation

**Example:**
```typescript
created_at: "2025-10-28T10:30:45.123Z"
```

**Use cases:**
- Sort summaries chronologically
- Determine summary freshness
- Audit trail

---

### Complete ConversationSummary Example

```typescript
const summary: ConversationSummary = {
  id: "1698501234567-abc123",
  session_id: "session-456",
  summary: "Patient John Doe, 75, has T2DM on Metformin 500mg BID. A1C: 7.2%. Discussed glucose monitoring, diet, and foot care. Caregiver: daughter Jane, visits 3x/week.",
  message_count: 48,
  start_message_id: "msg-1",
  end_message_id: "msg-48",
  importance_score: 8.5,
  created_at: "2025-10-28T10:30:45.123Z"
}
```

---

## Semantic Pinning

### SemanticPin (Lines 16-23)

```typescript
export interface SemanticPin {
  id: string
  session_id: string
  content: string
  source_message_id?: string
  importance_score: number
  pin_type: 'manual' | 'auto' | 'code' | 'concept' | 'system'
  created_at: string
}
```

**Purpose:**
"Pin" important information for always-available context, like sticky notes in a conversation.

---

#### What is Semantic Pinning?

**Concept:** Keep critical facts in every AI request, regardless of context window

**Example:**
```typescript
// Pinned fact
pin: "Patient is allergic to Penicillin"

// Every AI request includes:
context = [
  { role: 'system', content: 'Patient is allergic to Penicillin' },  // ‚Üê Pinned
  ...conversationHistory
]

// Even in a conversation 1000 messages later:
User: "I have a sinus infection"
AI: "I'll avoid Penicillin-based antibiotics since you're allergic..."
     ‚Üë AI remembers the pin, even though original message was 1000 messages ago
```

---

#### Property: id
```typescript
id: string
```

**What it is:** Unique pin identifier

---

#### Property: session_id
```typescript
session_id: string
```

**What it is:** Which session this pin belongs to

---

#### Property: content
```typescript
content: string
```

**What it is:** The pinned fact/information

**Examples:**
```typescript
content: "Patient allergic to Penicillin and Sulfa drugs"
content: "Glucose target range: 80-130 mg/dL fasting"
content: "Emergency contact: Jane Doe, 555-1234"
content: "Patient prefers large-print instructions"
```

**Best practices:**
- Concise (10-50 tokens)
- Factual (not opinions)
- Timeless (not "today he felt tired", but "has chronic fatigue")

---

#### Property: source_message_id
```typescript
source_message_id?: string
```

**What it is:** Optional reference to the original message

**Example:**
```typescript
// Original message
message: { id: "msg-42", text: "My dad is allergic to Penicillin" }

// Extracted pin
pin: {
  content: "Patient allergic to Penicillin",
  source_message_id: "msg-42"  // ‚Üê Links back to original
}
```

**Use cases:**
- Audit trail (where did this fact come from?)
- Update pins when source message is edited
- Allow users to "see original context"

---

#### Property: importance_score
```typescript
importance_score: number
```

**What it is:** Priority for context inclusion (0-10)

**Example:**
```typescript
importance_score: 10.0  // Critical (allergies, emergency contacts)
importance_score: 7.0   // High (medications, diagnoses)
importance_score: 4.0   // Medium (preferences, routines)
importance_score: 2.0   // Low (minor notes)
```

**Use case:** When context window is tight, keep high-importance pins
```typescript
const criticalPins = pins
  .filter(p => p.importance_score >= 8)
  .sort((a, b) => b.importance_score - a.importance_score)
```

---

#### Property: pin_type
```typescript
pin_type: 'manual' | 'auto' | 'code' | 'concept' | 'system'
```

**What it is:** How/why the pin was created

---

##### Pin Type Reference

| Type | Created By | Example |
|------|------------|---------|
| **manual** | User explicitly pins | User clicks "Pin this" button |
| **auto** | AI detects important info | AI recognizes allergy mention |
| **code** | System rule/trigger | Detecting medical terms |
| **concept** | Abstract concept extraction | "Patient prefers morning appointments" |
| **system** | Backend initialization | "Session created on 2025-10-28" |

---

##### manual
```typescript
pin_type: 'manual'
```

**When:** User explicitly saves information

**Example flow:**
```
User: "My dad is allergic to Penicillin"
AI: "I'll remember that. üîñ Pinned: Patient allergic to Penicillin"
     ‚Üë User clicked "Pin" button
```

**Use case:** User-driven knowledge base

---

##### auto
```typescript
pin_type: 'auto'
```

**When:** AI automatically detects important facts

**Example logic:**
```typescript
if (message.includes('allergic to')) {
  createPin({
    content: extractAllergy(message),
    pin_type: 'auto',
    importance_score: 10.0
  })
}
```

**Examples:**
- Allergies detected
- Medication names mentioned
- Emergency situations flagged

---

##### code
```typescript
pin_type: 'code'
```

**When:** Code snippet or technical information

**Example:**
```typescript
content: "Patient glucose tracking formula: (fasting + 2hr_post_meal) / 2"
pin_type: 'code'
```

**Use case:** Preserve calculations, formulas, or technical details

---

##### concept
```typescript
pin_type: 'concept'
```

**When:** Abstract concepts or patterns extracted

**Examples:**
```typescript
content: "Patient prefers morning medical appointments"
content: "Daughter handles all medical decisions"
content: "Patient dislikes needles, shows anxiety"
```

**Use case:** Behavioral patterns, preferences, soft facts

---

##### system
```typescript
pin_type: 'system'
```

**When:** System-generated metadata

**Examples:**
```typescript
content: "Session started: 2025-10-28"
content: "User subscribed to premium plan"
content: "HIPAA notice acknowledged"
```

---

#### Property: created_at
```typescript
created_at: string
```

**What it is:** When the pin was created

---

### Complete SemanticPin Example

```typescript
const allergyPin: SemanticPin = {
  id: "1698501234567-pin123",
  session_id: "session-456",
  content: "Patient allergic to Penicillin and Sulfa drugs. Severe reaction (anaphylaxis) in 2010.",
  source_message_id: "msg-42",
  importance_score: 10.0,
  pin_type: 'manual',  // User explicitly pinned
  created_at: "2025-10-28T10:30:45.123Z"
}
```

---

## Message Types

### MessageWithImportance (Lines 25-34)

```typescript
export interface MessageWithImportance {
  id: number
  session_id: string
  role: 'user' | 'assistant' | 'system'
  text: string
  model_id: string
  token_usage: number
  created_at: string
  importance_score: number
}
```

**Purpose:**
Extended message type with importance scoring for intelligent context selection.

---

#### Extends Base Message

This is the standard chat message with **one addition**: `importance_score`

**Base fields:**
- `id`, `session_id`, `role`, `text`, `created_at` ‚Üí Standard chat message
- `model_id`, `token_usage` ‚Üí Tracking metadata

**Added field:**
- `importance_score` ‚Üí For memory prioritization

---

#### Property: importance_score
```typescript
importance_score: number
```

**What it is:** How important this specific message is (0-10)

**Scoring logic:**
```typescript
function calculateImportance(message: Message): number {
  let score = 5.0  // Base score
  
  // Boost for medical terms
  if (hasMedicalTerms(message.text)) score += 2.0
  
  // Boost for questions
  if (message.text.includes('?')) score += 1.0
  
  // Boost for user messages (more important than AI)
  if (message.role === 'user') score += 1.0
  
  // Penalize short messages
  if (message.text.length < 20) score -= 1.0
  
  return Math.min(10, Math.max(0, score))
}
```

**Use case: Smart context window**
```typescript
// Include recent messages + high-importance old messages
const recentMessages = messages.slice(-20)  // Last 20 messages
const importantOldMessages = messages
  .slice(0, -20)
  .filter(m => m.importance_score >= 8)
  .slice(-5)  // Top 5 important old messages

const context = [...importantOldMessages, ...recentMessages]
```

---

### Complete MessageWithImportance Example

```typescript
const message: MessageWithImportance = {
  id: 42,
  session_id: "session-456",
  role: "user",
  text: "My dad is allergic to Penicillin, had anaphylaxis in 2010",
  model_id: "gpt-4-turbo",
  token_usage: 15,
  created_at: "2025-10-28T10:30:45.123Z",
  importance_score: 10.0  // Critical medical info
}
```

---

## Memory Context

### MemoryContext (Lines 36-41)

```typescript
export interface MemoryContext {
  recentMessages: MessageWithImportance[]
  semanticPins: SemanticPin[]
  summaries: ConversationSummary[]
  totalTokens: number
}
```

**Purpose:**
Complete context package sent to AI for each request.

---

#### What is MemoryContext?

**Concept:** Instead of sending ALL messages, send a smart selection:
1. **Recent messages** (last 10-20 messages, full text)
2. **Semantic pins** (critical facts, always included)
3. **Summaries** (condensed older conversation)

**Result:** Efficient use of context window

---

#### Property: recentMessages
```typescript
recentMessages: MessageWithImportance[]
```

**What it is:** Most recent messages (full text)

**Example:**
```typescript
recentMessages: [
  { id: 98, text: "What time should dad take his evening meds?", ... },
  { id: 99, text: "Evening medications should be taken at 7 PM...", ... },
  { id: 100, text: "Does he need to take them with food?", ... }
]
```

**Typical size:** 10-20 messages (500-1000 tokens)

---

#### Property: semanticPins
```typescript
semanticPins: SemanticPin[]
```

**What it is:** All pinned facts (always included)

**Example:**
```typescript
semanticPins: [
  { content: "Patient allergic to Penicillin", importance_score: 10.0, ... },
  { content: "Glucose target: 80-130 mg/dL", importance_score: 8.0, ... },
  { content: "Emergency contact: Jane 555-1234", importance_score: 9.0, ... }
]
```

**Typical size:** 5-15 pins (100-300 tokens)

---

#### Property: summaries
```typescript
summaries: ConversationSummary[]
```

**What it is:** Condensed versions of older conversation segments

**Example:**
```typescript
summaries: [
  { 
    summary: "Initial consultation: Patient diagnosed with T2DM, started on Metformin...",
    message_count: 50,
    ...
  },
  { 
    summary: "Follow-up: Discussed medication adherence, added daily glucose monitoring...",
    message_count: 35,
    ...
  }
]
```

**Typical size:** 2-5 summaries (200-500 tokens)

---

#### Property: totalTokens
```typescript
totalTokens: number
```

**What it is:** Total token count of entire context

**Calculation:**
```typescript
totalTokens = 
  countTokens(recentMessages) +
  countTokens(semanticPins) +
  countTokens(summaries)
```

**Use case:** Ensure context fits within model's limit
```typescript
const maxContextTokens = 8000  // Reserve space for response
if (memoryContext.totalTokens > maxContextTokens) {
  // Trim context (remove oldest summary, reduce recent messages, etc.)
}
```

---

### Complete MemoryContext Example

```typescript
const context: MemoryContext = {
  recentMessages: [
    { id: 98, text: "What time for evening meds?", importance_score: 7.0, ... },
    { id: 99, text: "Evening meds at 7 PM with dinner", importance_score: 6.0, ... }
  ],
  semanticPins: [
    { content: "Patient allergic to Penicillin", importance_score: 10.0, ... },
    { content: "Glucose target: 80-130 mg/dL", importance_score: 8.0, ... }
  ],
  summaries: [
    { 
      summary: "Patient John Doe, 75, T2DM on Metformin...",
      message_count: 50,
      ...
    }
  ],
  totalTokens: 1250  // 500 recent + 200 pins + 550 summary
}

// Send to AI:
const aiRequest = {
  messages: [
    { role: 'system', content: context.semanticPins.map(p => p.content).join('\n') },
    { role: 'system', content: context.summaries.map(s => s.summary).join('\n\n') },
    ...context.recentMessages.map(m => ({ role: m.role, content: m.text }))
  ],
  model: 'gpt-4-turbo'
}
```

---

## Request Types

### CreateSummaryRequest (Lines 46-51)

```typescript
export interface CreateSummaryRequest {
  session_id: string
  start_message_id: string
  end_message_id: string
  message_count: number
}
```

**Purpose:**
Request payload for `POST /api/memory/summaries` to create a new summary.

**Example:**
```typescript
const request: CreateSummaryRequest = {
  session_id: "session-456",
  start_message_id: "msg-1",
  end_message_id: "msg-50",
  message_count: 50
}

const response = await fetch('/api/memory/summaries', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify(request)
})

const summary: ConversationSummary = await response.json()
```

**Backend processing:**
1. Fetch messages 1-50
2. Send to AI: "Summarize this conversation"
3. Calculate importance score
4. Store summary in database

---

### CreatePinRequest (Lines 53-59)

```typescript
export interface CreatePinRequest {
  session_id: string
  content: string
  source_message_id?: string
  importance_score?: number
  pin_type?: 'manual' | 'auto' | 'code' | 'concept' | 'system'
}
```

**Purpose:**
Request payload for `POST /api/memory/pins` to create a new pin.

**Example:**
```typescript
const request: CreatePinRequest = {
  session_id: "session-456",
  content: "Patient allergic to Penicillin",
  source_message_id: "msg-42",
  importance_score: 10.0,
  pin_type: 'manual'
}

const response = await fetch('/api/memory/pins', {
  method: 'POST',
  body: JSON.stringify(request)
})

const pin: SemanticPin = await response.json()
```

**Defaults:**
- `importance_score`: 5.0 (if not provided)
- `pin_type`: 'manual' (if not provided)

---

## Response Types

### MemoryStats (Lines 64-71)

```typescript
export interface MemoryStats {
  totalMessages: number
  totalSummaries: number
  totalPins: number
  oldestMessage: string
  newestMessage: string
  averageImportanceScore: number
}
```

**Purpose:**
Analytics/overview of session memory state.

**Example endpoint:** `GET /api/memory/stats/:session_id`

**Example response:**
```typescript
const stats: MemoryStats = {
  totalMessages: 250,
  totalSummaries: 4,
  totalPins: 8,
  oldestMessage: "2025-10-01T08:00:00Z",
  newestMessage: "2025-10-28T10:30:45Z",
  averageImportanceScore: 6.3
}
```

**Use cases:**
- Dashboard widgets
- Session health monitoring
- User analytics ("You've had 250 conversations")

---

## Memory System Flow

### Complete Workflow Example

```typescript
// 1. User starts chat session
const session = await createSession({ name: "Dad's health" })

// 2. User sends messages
await sendMessage({ session_id: session.id, text: "My dad is allergic to Penicillin" })
await sendMessage({ session_id: session.id, text: "He takes Metformin 500mg twice daily" })
// ... 48 more messages ...

// 3. System creates summary after 50 messages
const summaryRequest: CreateSummaryRequest = {
  session_id: session.id,
  start_message_id: "msg-1",
  end_message_id: "msg-50",
  message_count: 50
}
const summary = await createSummary(summaryRequest)

// 4. System auto-pins critical info
const pinRequest: CreatePinRequest = {
  session_id: session.id,
  content: "Patient allergic to Penicillin",
  source_message_id: "msg-1",
  importance_score: 10.0,
  pin_type: 'auto'
}
const pin = await createPin(pinRequest)

// 5. User sends new message
const newMessage = "What antibiotics are safe?"

// 6. System builds context
const context: MemoryContext = {
  recentMessages: getRecentMessages(session.id, 20),
  semanticPins: getAllPins(session.id),
  summaries: getAllSummaries(session.id),
  totalTokens: 1500
}

// 7. Send to AI with context
const aiResponse = await generateResponse({
  messages: [
    { role: 'system', content: context.semanticPins.map(p => p.content).join('\n') },
    { role: 'system', content: context.summaries.map(s => s.summary).join('\n\n') },
    ...context.recentMessages.map(m => ({ role: m.role, content: m.text })),
    { role: 'user', content: newMessage }
  ]
})

// 8. AI remembers pinned allergy even though original message was 50+ messages ago
console.log(aiResponse.reply)
// "Since you're allergic to Penicillin, safe alternatives include Azithromycin..."
```

---

## Best Practices

### 1. Create Summaries Regularly
```typescript
// ‚úÖ Good - summarize every 50 messages
if (messageCount % 50 === 0) {
  createSummary({ session_id, start_message_id, end_message_id, message_count: 50 })
}

// ‚ùå Bad - wait until context window is full
if (contextTokens > 120000) {  // Too late!
  createSummary(...)
}
```

### 2. Pin Sparingly
```typescript
// ‚úÖ Good - only pin critical, timeless facts
pins: [
  "Patient allergic to Penicillin",
  "Glucose target: 80-130 mg/dL",
  "Emergency contact: Jane 555-1234"
]

// ‚ùå Bad - pin everything
pins: [
  "Patient felt tired today",
  "It's sunny outside",
  "AI said hello"
]
```

### 3. Score Importance Consistently
```typescript
// ‚úÖ Good - consistent scoring rubric
10.0: Life-threatening (allergies, emergency contacts)
8.0-9.0: Critical medical (diagnoses, medications)
6.0-7.0: Important context (preferences, history)
4.0-5.0: Useful notes (routines, observations)
1.0-3.0: Minor details (casual chat, greetings)

// ‚ùå Bad - random scores
importance_score: Math.random() * 10
```

### 4. Monitor Token Usage
```typescript
// ‚úÖ Good - track and limit
const MAX_CONTEXT_TOKENS = 8000
if (memoryContext.totalTokens > MAX_CONTEXT_TOKENS) {
  memoryContext = trimContext(memoryContext, MAX_CONTEXT_TOKENS)
}

// ‚ùå Bad - send everything
const context = { recentMessages, semanticPins, summaries, totalTokens: 50000 }
// Context window exceeded! üí•
```

---

## Summary

**memory.ts defines Kalito's Phase 2 memory system:**

### Core Types
- **ConversationSummary**: Condensed conversation segments
- **SemanticPin**: Critical facts always included in context
- **MessageWithImportance**: Messages with importance scoring
- **MemoryContext**: Complete context package for AI

### Request/Response Types
- **CreateSummaryRequest**: Create new summary
- **CreatePinRequest**: Pin important information
- **MemoryStats**: Session memory analytics

**Key benefits:**
- **Efficient context management** (summaries save tokens)
- **Never forget critical facts** (semantic pins)
- **Smart message selection** (importance scoring)
- **Scalable conversations** (works with 1000+ messages)

**Memory system enables:**
- Long-running conversations without context loss
- Automatic extraction of critical information
- Token-efficient context windows
- Personalized, context-aware AI responses
