# Agent Service Documentation

## Overview

The `agentService.ts` file is the **core orchestration layer** for AI agent interactions in the Kalito application. It manages the complete lifecycle of AI conversations, from building context-aware prompts to executing function calls and streaming responses.

**Location**: `/backend/logic/agentService.ts`

**Primary Responsibilities**:
- Orchestrate AI model interactions (OpenAI, Ollama)
- Build intelligent, context-aware system prompts
- Manage conversation history with hybrid memory system
- Execute function calls (web search via Tavily)
- Support both streaming and non-streaming responses
- Integrate eldercare context dynamically
- Apply persona-based customization

**Key Features**:
- **Hybrid Memory System**: Combines recent messages, conversation summaries, and semantic pins
- **Function Calling**: Automatic web search integration for cloud models
- **Eldercare Context**: Dynamic integration of patient, medication, and caregiver data
- **Persona Support**: Customizable AI personalities with model-specific settings
- **Streaming Support**: Real-time response generation
- **Model Agnostic**: Works with both OpenAI (cloud) and Ollama (local) models

---

## Architecture Overview

### Component Dependencies

```typescript
import { getModelAdapter } from './modelRegistry'
import type { LLMAdapter } from './modelRegistry'
import { db } from '../db/db'
import { MemoryManager } from './memoryManager'
import { EldercareContextService } from './eldercareContextService'
import { AVAILABLE_TOOLS, executeToolCall } from './tools'
```

**Dependency Graph**:
```
agentService.ts
‚îú‚îÄ‚îÄ modelRegistry.ts (model adapter selection)
‚îú‚îÄ‚îÄ memoryManager.ts (conversation history & context)
‚îú‚îÄ‚îÄ eldercareContextService.ts (patient/medication data)
‚îú‚îÄ‚îÄ tools.ts (function calling - web search)
‚îî‚îÄ‚îÄ db.ts (persona settings, conversation history fallback)
```

**Service Initialization**:
```typescript
const memoryManager = new MemoryManager()
const eldercareContextService = new EldercareContextService()
```

**Singleton Pattern**: Services initialized once at module load for efficiency.

---

## Core Data Types

### RunAgentParams

```typescript
type RunAgentParams = AgentRequest & {
  stream?: boolean      // Enable streaming mode
  personaId?: string    // Persona selection for customization
  sessionId?: string    // Session ID for conversation history
}
```

**Extended from AgentRequest**:
```typescript
interface AgentRequest {
  input: string                  // User's message
  systemPrompt?: string          // Custom system instructions
  modelName: string              // Model identifier (e.g., 'gpt-4o-mini')
  settings: {
    model: string
    temperature?: number         // 0.1 - 1.2
    maxTokens?: number          // 1 - 4000
    topP?: number               // 0 - 1
    frequencyPenalty?: number   // 0 - 2
    presencePenalty?: number    // 0 - 2
    outputFormat?: string       // 'json', 'text', etc.
  }
  fileIds?: string[]            // Document IDs (future feature)
}
```

**Purpose**: Unified parameter type for both streaming and non-streaming agent execution.

---

## Helper Functions

### 1. getPersonaPrompt

```typescript
function getPersonaPrompt(personaId?: string): string
```

**Purpose**: Retrieves persona's system prompt from database.

**Implementation**:
```typescript
function getPersonaPrompt(personaId?: string): string {
  if (!personaId) return ''

  try {
    const persona = db.prepare('SELECT prompt FROM personas WHERE id = ?').get(personaId) as
      | { prompt: string }
      | undefined

    return persona?.prompt || ''
  } catch (error) {
    console.error('Error fetching persona:', error)
    return ''
  }
}
```

**Behavior**:
- Returns empty string if no `personaId` provided
- Returns empty string if persona not found
- Returns empty string on database error (logs error)
- Otherwise returns persona's system prompt

**Database Query**:
```sql
SELECT prompt FROM personas WHERE id = ?
```

**Error Handling**: Graceful degradation - returns empty string instead of throwing.

**Usage Example**:
```typescript
const prompt = getPersonaPrompt('persona_eldercare_nurse')
// Returns: "You are a compassionate eldercare nurse with 20 years of experience..."
```

---

### 2. getPersonaSettings

```typescript
function getPersonaSettings(personaId?: string): Record<string, unknown> | undefined
```

**Purpose**: Retrieves and normalizes persona's model settings for both local (Ollama) and cloud (OpenAI) adapters.

**Implementation**:
```typescript
function getPersonaSettings(personaId?: string): Record<string, unknown> | undefined {
  if (!personaId) return undefined

  try {
    const persona = db.prepare(
      'SELECT temperature, maxTokens, topP, repeatPenalty FROM personas WHERE id = ?'
    ).get(personaId) as {
      temperature?: number
      maxTokens?: number  
      topP?: number
      repeatPenalty?: number
    } | undefined

    if (!persona) return undefined

    const settings: Record<string, unknown> = {}
    
    if (persona.temperature != null) settings.temperature = persona.temperature
    if (persona.maxTokens != null) settings.maxTokens = persona.maxTokens
    if (persona.topP != null) settings.topP = persona.topP

    // üîë Normalize repeatPenalty for both local + OpenAI
    if (persona.repeatPenalty != null) {
      settings.repeatPenalty = persona.repeatPenalty    // used by Ollama
      settings.frequencyPenalty = persona.repeatPenalty // used by OpenAI
    }

    return Object.keys(settings).length > 0 ? settings : undefined
  } catch (error) {
    console.error('Error fetching persona settings:', error)
    return undefined
  }
}
```

**Database Query**:
```sql
SELECT temperature, maxTokens, topP, repeatPenalty FROM personas WHERE id = ?
```

**Settings Normalization**:

| Persona Field | Ollama Setting | OpenAI Setting | Range |
|---------------|----------------|----------------|-------|
| `temperature` | `temperature` | `temperature` | 0.1 - 1.2 |
| `maxTokens` | `maxTokens` | `maxTokens` | 1 - 4000 |
| `topP` | `topP` | `topP` | 0 - 1 |
| `repeatPenalty` | `repeatPenalty` | `frequencyPenalty` | 0 - 2 |

**Key Feature**: **Cross-Platform Compatibility**
- Maps `repeatPenalty` to both `repeatPenalty` (Ollama) and `frequencyPenalty` (OpenAI)
- Ensures personas work seamlessly across local and cloud models

**Return Value**:
- `undefined` if no personaId or persona not found
- `undefined` if no settings configured (all null)
- Settings object with only configured fields

**Usage Example**:
```typescript
const settings = getPersonaSettings('persona_eldercare_nurse')
// Returns: { temperature: 0.7, maxTokens: 2000, repeatPenalty: 0.5, frequencyPenalty: 0.5 }

// Merges with request settings
const merged = { ...settings, ...requestSettings }
```

---

### 3. getDocumentContext

```typescript
function getDocumentContext(_fileIds: string[] = []): string
```

**Purpose**: Placeholder for future document context feature (RAG - Retrieval Augmented Generation).

**Implementation**:
```typescript
function getDocumentContext(_fileIds: string[] = []): string {
  return ''
}
```

**Current Status**: Not implemented - always returns empty string.

**Future Implementation**:
- Will retrieve document content by file IDs
- Enable RAG (Retrieval Augmented Generation)
- Support document-based Q&A
- Integrate with vector database for semantic search

**Usage**: Currently a no-op, designed for future extension.

---

### 4. getConversationHistory

```typescript
async function getConversationHistory(
  sessionId?: string, 
  maxTokens: number = 3000
): Promise<Array<{ role: string; content: string }>>
```

**Purpose**: Builds enhanced conversation history using hybrid memory system with intelligent context optimization.

**This is the most complex and critical function in the service.**

#### Architecture

**Hybrid Memory Components**:
1. **Recent Messages**: Last 10-11 messages from session
2. **Conversation Summaries**: AI-generated summaries of older exchanges
3. **Semantic Pins**: User-flagged important information
4. **Token-Aware**: Respects `maxTokens` budget for context window

#### Implementation (Detailed Breakdown)

**Step 1: Get Enhanced Context from MemoryManager**

```typescript
const memoryContext = await memoryManager.buildContext(sessionId, maxTokens)
```

**MemoryContext Structure**:
```typescript
{
  recentMessages: MessageWithImportance[],  // Last N messages with importance scores
  summaries: ConversationSummary[],         // Older conversation summaries
  semanticPins: SemanticPin[],              // User-pinned important info
  totalTokens: number                       // Estimated token count
}
```

**Step 2: Initialize History Array**

```typescript
const history: Array<{ role: string; content: string }> = []
```

**Step 3: Add Critical Instruction (Anti-Hallucination)**

```typescript
history.push({
  role: 'system',
  content: 'CRITICAL INSTRUCTION: The following is REFERENCE material from previous conversation. Use it ONLY for contextual understanding. You MUST FOCUS EXCLUSIVELY on responding to the CURRENT QUESTION. DO NOT address any previous topics or questions unless the current question EXPLICITLY requests information about them. Previous exchanges should be treated as background information only.'
})
```

**Purpose**: Prevents model from answering old questions instead of current query.

**Step 4: Add Conversation Summaries**

```typescript
if (memoryContext.summaries.length > 0) {
  const summariesText = memoryContext.summaries
    .map((summary: ConversationSummary) => `${summary.summary}`)
    .join('\n\n')
  
  history.push({
    role: 'system',
    content: `REFERENCE ONLY - Background context: ${summariesText} [NOTE: This is background information only. Do not directly respond to any of these topics unless explicitly asked in the current question.]`
  })
}
```

**Summaries Format**:
- Concatenated with double newlines
- Labeled as "REFERENCE ONLY"
- Explicit warning not to respond to summary content

**Step 5: Add Semantic Pins**

```typescript
if (memoryContext.semanticPins.length > 0) {
  const pinsText = memoryContext.semanticPins
    .map((pin: SemanticPin) => pin.content)
    .join('; ')
  
  history.push({
    role: 'system', 
    content: `REFERENCE MATERIAL - Important information: ${pinsText} [Use this information only if directly relevant to the current question.]`
  })
}
```

**Pins Format**:
- Concatenated with semicolons
- Labeled as "REFERENCE MATERIAL"
- Conditional usage instruction

**Step 6: Exclude Current Message**

```typescript
const messages = memoryContext.recentMessages.slice(0, -1)
```

**Why**: The current user input is added separately later - don't duplicate it.

**Step 7: Divide Messages into Groups**

```typescript
const totalMessages = messages.length
const recentThreshold = Math.min(2, totalMessages)  // Last 2 exchanges as "recent"
```

**Strategy**: Treat last 2 messages differently than older messages.

**Step 8: Add Older Messages as Condensed Reference**

```typescript
if (totalMessages > recentThreshold) {
  const olderMessages = messages.slice(0, totalMessages - recentThreshold)
  
  if (olderMessages.length > 0) {
    // Filter out invalid roles
    const validOlderMessages = olderMessages.filter((msg: MessageWithImportance) => {
      const validRoles = ['system', 'user', 'assistant']
      if (!validRoles.includes(msg.role)) {
        console.warn(`[Agent] Filtering out older message with invalid role: ${msg.role}`)
        return false
      }
      return true
    })
    
    if (validOlderMessages.length > 0) {
      const combinedOlderMessages = validOlderMessages
        .map((msg: MessageWithImportance) => `${msg.role.toUpperCase()}: ${msg.text}`)
        .join('\n\n')
        
      history.push({
        role: 'system',
        content: `HISTORICAL REFERENCE ONLY: ${combinedOlderMessages}\n\n[NOTE: These are older messages provided only for background context. DO NOT respond to any questions or requests in this historical content.]`
      })
    }
  }
  
  // Separator
  history.push({
    role: 'system',
    content: '--- RECENT CONVERSATION ---'
  })
}
```

**Older Messages Strategy**:
- Combined into single system message
- Labeled "HISTORICAL REFERENCE ONLY"
- Explicit warning not to respond
- Role validation (only system/user/assistant)
- Visual separator added

**Step 9: Add Recent Messages Individually**

```typescript
const recentMessages = messages.slice(Math.max(0, totalMessages - recentThreshold))

for (const message of recentMessages) {
  // Validate role
  const validRoles = ['system', 'user', 'assistant']
  if (!validRoles.includes(message.role)) {
    console.warn(`[Agent] Skipping message with invalid role: ${message.role}`)
    continue
  }
  
  // Add with prefix
  history.push({
    role: message.role,
    content: `RECENT EXCHANGE: ${message.text}\n[This is from the recent conversation, not the current question.]`
  })
}

return history
```

**Recent Messages Strategy**:
- Added as individual messages (preserves role structure)
- Labeled "RECENT EXCHANGE"
- Note that it's not the current question
- Role validation

#### Error Handling & Fallback

```typescript
catch (error) {
  console.error('Error building enhanced conversation history:', error)
  try {
    // Fallback: Simple database query
    const rows = db
      .prepare(`
        SELECT * FROM (
          SELECT role, text, created_at FROM messages 
          WHERE session_id = ? 
          ORDER BY created_at DESC 
          LIMIT 11
        ) ORDER BY created_at ASC
      `)
      .all(sessionId) as { role: string; text: string }[]

    const trimmed = rows.slice(0, -1)
    return trimmed.map(msg => ({
      role: msg.role,
      content: msg.text
    }))
  } catch (fallbackError) {
    console.error('Error in fallback conversation history:', fallbackError)
    return []
  }
}
```

**Fallback Strategy**:
1. If MemoryManager fails, fall back to simple database query
2. Get last 11 messages, ordered chronologically
3. Remove last message (current query)
4. Return simple format without enhancements
5. If fallback also fails, return empty array

**Fallback Query**:
```sql
SELECT * FROM (
  SELECT role, text, created_at FROM messages 
  WHERE session_id = ? 
  ORDER BY created_at DESC 
  LIMIT 11
) ORDER BY created_at ASC
```

**Why Nested Query**: Inner query gets newest 11, outer orders them chronologically.

#### Complete History Structure Example

```typescript
[
  {
    role: 'system',
    content: 'CRITICAL INSTRUCTION: The following is REFERENCE material...'
  },
  {
    role: 'system',
    content: 'REFERENCE ONLY - Background context: The user discussed their grandmother\'s medication schedule...'
  },
  {
    role: 'system',
    content: 'REFERENCE MATERIAL - Important information: Patient takes 10mg Lisinopril daily at 8am; Allergic to penicillin'
  },
  {
    role: 'system',
    content: 'HISTORICAL REFERENCE ONLY: USER: What medications does grandma take?\nASSISTANT: Your grandmother takes...'
  },
  {
    role: 'system',
    content: '--- RECENT CONVERSATION ---'
  },
  {
    role: 'user',
    content: 'RECENT EXCHANGE: When should I give her the blood pressure medicine?\n[This is from the recent conversation, not the current question.]'
  },
  {
    role: 'assistant',
    content: 'RECENT EXCHANGE: The Lisinopril should be given at 8am each morning...\n[This is from the recent conversation, not the current question.]'
  }
]
```

**Token Optimization**: MemoryManager ensures total doesn't exceed `maxTokens` (default 3000).

---

### 5. buildSystemPrompt

```typescript
function buildSystemPrompt(
  adapter: LLMAdapter,
  userInput: string,
  personaId?: string,
  customSystemPrompt?: string,
  fileIds: string[] = []
): string
```

**Purpose**: Constructs the final system prompt by combining multiple context sources.

**Implementation**:
```typescript
function buildSystemPrompt(
  adapter: LLMAdapter,
  userInput: string,
  personaId?: string,
  customSystemPrompt?: string,
  fileIds: string[] = []
): string {
  const personaPrompt = getPersonaPrompt(personaId)
  const documentContext = getDocumentContext(fileIds)
  const customPrompt = customSystemPrompt?.trim() || ''
  
  // Get eldercare context based on user query and model capabilities
  const eldercareContext = eldercareContextService.generateContextualPrompt(adapter, userInput)
  
  // Add stronger and more explicit instructions to focus on current query only
  const focusInstructions = "CRITICAL INSTRUCTION: You MUST address ONLY the user's CURRENT QUESTION. Previous conversation is provided SOLELY as background context. You MUST NOT: 1) Answer questions from previous exchanges, 2) Refer to previous topics unless explicitly asked, 3) Provide information not directly relevant to the current question. Treat the current question as if it were asked in isolation, while using context only to enhance your understanding of what the user is currently asking."

  const parts = [personaPrompt, documentContext, eldercareContext, customPrompt, focusInstructions].filter(Boolean)
  return parts.join('\n\n')
}
```

**System Prompt Components** (in order):

| Component | Source | Purpose | Example |
|-----------|--------|---------|---------|
| Persona Prompt | `getPersonaPrompt()` | AI personality/role | "You are a compassionate nurse..." |
| Document Context | `getDocumentContext()` | RAG context | "" (not implemented) |
| Eldercare Context | `eldercareContextService` | Patient/med data | "Patient: Mary Smith, Medications: [...]" |
| Custom Prompt | Request parameter | User customization | "Be brief and concise" |
| Focus Instructions | Hardcoded | Anti-hallucination | "You MUST address ONLY the user's CURRENT QUESTION..." |

**Assembly Strategy**:
1. Collect all components
2. Filter out empty/falsy values with `.filter(Boolean)`
3. Join with double newlines (`\n\n`)

**Eldercare Context Integration**:
```typescript
const eldercareContext = eldercareContextService.generateContextualPrompt(adapter, userInput)
```

**Dynamic Context**: `EldercareContextService` analyzes the user input and adapter capabilities to determine which eldercare data to include.

**Focus Instructions Purpose**:
- Prevents model from answering old questions
- Ensures current question takes priority
- Critical for multi-turn conversations
- Reduces hallucinations

**Example Output**:
```
You are a compassionate eldercare nurse with 20 years of experience specializing in dementia care and medication management.

Patient Information:
- Name: Mary Smith
- Age: 82
- Conditions: Hypertension, Type 2 Diabetes

Current Medications:
- Lisinopril 10mg - Once daily at 8am
- Metformin 500mg - Twice daily with meals

Be brief and concise in your responses.

CRITICAL INSTRUCTION: You MUST address ONLY the user's CURRENT QUESTION. Previous conversation is provided SOLELY as background context...
```

---

## Main Functions

### runAgent (Non-Streaming)

```typescript
export async function runAgent(
  payload: RunAgentParams
): Promise<{ reply: string; tokenUsage: number | null }>
```

**Purpose**: Executes AI agent for a single, complete (non-streaming) response with automatic function calling support.

#### Implementation Flow

**Step 1: Extract Parameters**

```typescript
const { modelName, settings, input, systemPrompt = '', fileIds = [], personaId, sessionId } = payload
```

**Step 2: Get Model Adapter**

```typescript
const adapter = getModelAdapter(modelName)
if (!adapter) throw new Error(`Adapter for model "${modelName}" not found.`)
```

**Throws**: If model adapter not found in registry.

**Step 3: Merge Settings**

```typescript
const personaSettings = getPersonaSettings(personaId)
const mergedSettings = { ...personaSettings, ...settings }

console.log('üîç PERSONA SETTINGS DEBUG (runAgent):', {
  personaId,
  personaSettings,
  requestSettings: settings,
  mergedSettings
})
```

**Merge Strategy**: Persona settings as base, request settings override.

**Step 4: Build System Prompt**

```typescript
const finalSystemPrompt = buildSystemPrompt(adapter, input, personaId, systemPrompt, fileIds)
```

**Step 5: Construct Messages Array**

```typescript
const messages: Array<{ role: string; content: string }> = []

// Add system prompt
if (finalSystemPrompt && finalSystemPrompt.trim()) {
  messages.push({ role: 'system', content: finalSystemPrompt.trim() })
}

// Add conversation history
if (sessionId) {
  const history = await getConversationHistory(sessionId, 3000)
  history.forEach(msg => messages.push({ role: msg.role, content: msg.content }))
}

// Add current user input
if (input && input.trim()) {
  messages.push({ role: 'user', content: input.trim() })
}
```

**Message Order**:
1. System prompt (personality, context, instructions)
2. Conversation history (summaries, pins, messages)
3. Current user input

**Step 6: Setup Tools (Cloud Models Only)**

```typescript
const tools: ChatCompletionTool[] | undefined = 
  adapter.type === 'cloud' 
    ? ([...AVAILABLE_TOOLS] as unknown as ChatCompletionTool[]) 
    : undefined

console.log(`[Agent] Using ${tools ? tools.length : 0} tools for model type: ${adapter.type}`)
```

**Tool Availability**:
- **Cloud (OpenAI)**: Full function calling support
- **Local (Ollama)**: No tools (not reliably supported)

**Step 7: Initial Generation**

```typescript
let result = await adapter.generate({ 
  messages, 
  settings: mergedSettings, 
  fileIds, 
  tools 
})
```

**Result Format**:
```typescript
{
  reply: string,
  tokenUsage: number | null,
  toolCalls?: Array<{
    id: string,
    name: string,
    arguments: string  // JSON string
  }>
}
```

#### Function Calling (Tool Execution)

**Step 8: Handle Tool Calls (If Any)**

```typescript
if (result.toolCalls && result.toolCalls.length > 0) {
  console.log(`[Agent] Model requested ${result.toolCalls.length} tool call(s)`)

  let searchResults = ''
  
  for (const toolCall of result.toolCalls) {
    console.log(`[Agent] Executing tool: ${toolCall.name}`)
    
    try {
      const args = JSON.parse(toolCall.arguments)
      const toolResult = await executeToolCall(toolCall.name, args)
      searchResults += `Search results for "${args.query}":\n${toolResult}\n\n`
    } catch (error) {
      console.error(`[Agent] Tool execution failed:`, error)
      searchResults += `Search failed: ${error instanceof Error ? error.message : 'Unknown error'}\n\n`
    }
  }
```

**Tool Execution Flow**:
1. Parse tool call arguments (JSON string ‚Üí object)
2. Execute tool via `executeToolCall()`
3. Collect results as formatted text
4. Handle errors gracefully (continue on failure)

**Step 9: Generate Final Response with Tool Results**

```typescript
  // Create new request with search results as context
  const newSystemPrompt = `${finalSystemPrompt}\n\nYou have access to current search results. Use them to provide accurate, up-to-date information.`
  
  const newMessages: Array<{ role: string; content: string }> = [
    { role: 'system', content: newSystemPrompt },
    { role: 'system', content: `Current search results:\n${searchResults}` },
    { role: 'user', content: input.trim() }
  ]

  // Generate final response (no tools to avoid recursion)
  console.log(`[Agent] Generating final response with search results`)
  result = await adapter.generate({ 
    messages: newMessages, 
    settings: mergedSettings, 
    fileIds 
  })
  
  // Add marker for frontend
  result.reply = '[SEARCHING_ONLINE]\n\n' + result.reply
}
```

**Second Generation Strategy**:
- Create fresh messages array (avoid context bloat)
- Include original system prompt + search results
- Include only current user input (not full history)
- No tools in second call (prevents infinite recursion)
- Add `[SEARCHING_ONLINE]` marker for UI

**Step 10: Return Result**

```typescript
return result
```

#### Complete Function Call Example

**User Input**: "What's the weather in San Francisco?"

**Flow**:
1. Model generates: `{ toolCalls: [{ name: 'tavily_search', arguments: '{"query": "weather San Francisco"}' }] }`
2. Execute tool: `executeToolCall('tavily_search', { query: 'weather San Francisco' })`
3. Get results: `"Current weather in SF: 65¬∞F, partly cloudy..."`
4. Second generation with results as context
5. Final response: `"[SEARCHING_ONLINE]\n\nThe current weather in San Francisco is 65¬∞F and partly cloudy..."`

---

### runAgentStream (Streaming)

```typescript
export async function* runAgentStream(
  payload: RunAgentParams
): AsyncGenerator<{ delta: string; done?: boolean; tokenUsage?: number }>
```

**Purpose**: Executes AI agent in streaming mode for real-time response generation with function calling support.

**Return Type**: `AsyncGenerator` that yields chunks of response as they arrive.

#### Implementation Flow

**Step 1-5: Same as runAgent**

```typescript
const { modelName, settings, input, systemPrompt = '', fileIds = [], personaId, sessionId } = payload
const adapter = getModelAdapter(modelName)
if (!adapter) throw new Error(`Adapter for model "${modelName}" not found.`)

const personaSettings = getPersonaSettings(personaId)
const mergedSettings = { ...personaSettings, ...settings }
    
const finalSystemPrompt = buildSystemPrompt(adapter, input, personaId, systemPrompt, fileIds)

const messages = []
if (finalSystemPrompt && finalSystemPrompt.trim()) {
  messages.push({ role: 'system', content: finalSystemPrompt.trim() })
}

if (sessionId) {
  const history = await getConversationHistory(sessionId, 3000)
  history.forEach(msg => messages.push({ role: msg.role, content: msg.content }))
}

if (input && input.trim()) {
  messages.push({ role: 'user', content: input.trim() })
}
```

**Step 6: Validate Streaming Support**

```typescript
if (!('generateStream' in adapter) || typeof adapter.generateStream !== 'function') {
  throw new Error(`Adapter "${modelName}" does not support streaming.`)
}
```

**Runtime Check**: Ensures adapter implements streaming method.

**Step 7: Setup Tools**

```typescript
const tools: ChatCompletionTool[] | undefined = 
  adapter.type === 'cloud' 
    ? ([...AVAILABLE_TOOLS] as unknown as ChatCompletionTool[]) 
    : undefined

console.log(`[Agent Stream] Using ${tools ? tools.length : 0} tools for model type: ${adapter.type}`)
```

#### Function Calling in Streaming Mode

**Challenge**: OpenAI's streaming API doesn't support tool calls in streaming mode reliably.

**Solution**: Hybrid approach - non-streaming check for tools, then stream final response.

**Step 8: Check for Tool Calls (Non-Streaming)**

```typescript
if (tools && tools.length > 0) {
  // First, do a non-streaming call to check for tool calls
  const initialResult = await adapter.generate({ 
    messages, 
    settings: mergedSettings, 
    fileIds, 
    tools 
  })
  
  if (initialResult.toolCalls && initialResult.toolCalls.length > 0) {
    // Yield searching marker
    yield { delta: '[SEARCHING_ONLINE]\n\n' }
```

**Why Non-Streaming First?**
- Detects tool calls reliably
- Avoids complex streaming + function calling race conditions
- Provides immediate feedback to user

**Step 9: Execute Tools**

```typescript
    // Execute tool calls
    const toolResults = await Promise.all(
      initialResult.toolCalls.map(async (toolCall) => {
        const result = await executeToolCall(
          toolCall.name, 
          JSON.parse(toolCall.arguments)
        )
        return { toolCallId: toolCall.id, result }
      })
    )

    // Build search results context
    const searchResults = toolResults.map(tr => tr.result).join('\n\n')
```

**Parallel Execution**: Uses `Promise.all` for concurrent tool calls.

**Step 10: Stream Final Response**

```typescript
    // Create new messages with search results
    const newSystemPrompt = `${finalSystemPrompt}\n\nYou have access to current search results. Use them to provide accurate, up-to-date information.`
    
    const newMessages: Array<{ role: string; content: string }> = [
      { role: 'system', content: newSystemPrompt },
      { role: 'system', content: `Current search results:\n${searchResults}` },
      { role: 'user', content: input.trim() }
    ]

    // Stream final response
    for await (const chunk of adapter.generateStream({ 
      messages: newMessages, 
      settings: mergedSettings, 
      fileIds 
    })) {
      yield chunk
    }
    return
  }
}
```

**Stream Response Format**:
```typescript
{
  delta: string,      // Text chunk
  done?: boolean,     // True on last chunk
  tokenUsage?: number // Token count (optional, last chunk only)
}
```

**Step 11: Normal Streaming (No Tool Calls)**

```typescript
// No tool calls, proceed with normal streaming
for await (const chunk of adapter.generateStream({ 
  messages, 
  settings: mergedSettings, 
  fileIds 
})) {
  yield chunk
}
```

#### Streaming Example

**User Input**: "What's the current Bitcoin price?"

**Streaming Flow**:
```
1. yield { delta: '[SEARCHING_ONLINE]\n\n' }
2. (Execute tavily_search tool)
3. yield { delta: 'The' }
4. yield { delta: ' current' }
5. yield { delta: ' Bitcoin' }
6. yield { delta: ' price' }
7. yield { delta: ' is' }
8. yield { delta: ' $43,250' }
9. yield { delta: '.', done: true, tokenUsage: 45 }
```

**Frontend Display**: Text appears progressively as chunks arrive.

---

### getAllSessions

```typescript
export async function getAllSessions()
```

**Purpose**: Retrieves all chat sessions from database, ordered newest first.

**Implementation**:
```typescript
export async function getAllSessions() {
  const rows = db
    .prepare('SELECT id, name, created_at FROM sessions ORDER BY created_at DESC')
    .all()
  return rows
}
```

**Database Query**:
```sql
SELECT id, name, created_at 
FROM sessions 
ORDER BY created_at DESC
```

**Return Format**:
```typescript
Array<{
  id: string,
  name: string,
  created_at: string  // ISO timestamp
}>
```

**Usage**: Session list in UI sidebar.

---

## Complete Agent Execution Flow

### Non-Streaming Flow

```
1. User sends message
   ‚Üì
2. Get model adapter (OpenAI/Ollama)
   ‚Üì
3. Merge persona settings + request settings
   ‚Üì
4. Build system prompt
   - Persona prompt
   - Eldercare context (dynamic)
   - Custom prompt
   - Focus instructions
   ‚Üì
5. Build messages array
   - System prompt
   - Conversation history (hybrid memory)
   - Current user input
   ‚Üì
6. Generate with tools (cloud only)
   ‚Üì
7. Check for tool calls
   ‚îú‚îÄ No ‚Üí Return response
   ‚îî‚îÄ Yes ‚Üí Execute tools
       ‚Üì
       8. Collect tool results
       ‚Üì
       9. Generate final response with results
       ‚Üì
       10. Add [SEARCHING_ONLINE] marker
       ‚Üì
       11. Return response
```

### Streaming Flow

```
1. User sends message
   ‚Üì
2-5. (Same as non-streaming)
   ‚Üì
6. Check adapter supports streaming
   ‚Üì
7. If tools available:
   ‚îú‚îÄ Do non-streaming call to check for tools
   ‚îú‚îÄ If tool calls found:
   ‚îÇ  ‚îú‚îÄ Yield [SEARCHING_ONLINE] marker
   ‚îÇ  ‚îú‚îÄ Execute tools (parallel)
   ‚îÇ  ‚îú‚îÄ Stream final response with results
   ‚îÇ  ‚îî‚îÄ Return
   ‚îî‚îÄ If no tool calls:
       ‚îî‚îÄ Proceed to normal streaming
   ‚Üì
8. Stream response chunks
   ‚Üì
9. Yield each chunk as it arrives
   ‚Üì
10. Yield final chunk with done=true
```

---

## Function Calling Details

### Available Tools

**Current Tools** (from `tools.ts`):
- `tavily_search` - Web search via Tavily API

**Tool Definition** (OpenAI format):
```typescript
{
  type: 'function',
  function: {
    name: 'tavily_search',
    description: 'Search the web for current information',
    parameters: {
      type: 'object',
      properties: {
        query: {
          type: 'string',
          description: 'The search query'
        }
      },
      required: ['query']
    }
  }
}
```

### Tool Execution

**executeToolCall Implementation** (from `tools.ts`):
```typescript
export async function executeToolCall(name: string, args: any): Promise<string> {
  switch (name) {
    case 'tavily_search':
      return await tavilySearch(args.query)
    default:
      throw new Error(`Unknown tool: ${name}`)
  }
}
```

**Tool Result Format**: Plain text string (formatted for model consumption).

### Why Cloud Only?

**OpenAI (Cloud)**:
- Native function calling support
- Reliable tool call detection
- Well-documented API
- JSON mode support

**Ollama (Local)**:
- Function calling support is experimental
- Inconsistent across models
- Requires manual prompt engineering
- Not production-ready

**Decision**: Disable tools for local models to ensure reliability.

---

## Memory System Integration

### Hybrid Memory Architecture

**Components**:
1. **MemoryManager** - Orchestrates all memory operations
2. **Recent Messages** - Last 10-11 messages (recency)
3. **Conversation Summaries** - AI-generated summaries (compression)
4. **Semantic Pins** - User-flagged important info (relevance)

**Token Budget**: Default 3000 tokens for conversation history.

### Memory Building Process

```
User Query
  ‚Üì
MemoryManager.buildContext(sessionId, maxTokens=3000)
  ‚Üì
1. Get recent messages from database
  ‚Üì
2. Get conversation summaries (if exist)
  ‚Üì
3. Get semantic pins (if exist)
  ‚Üì
4. Estimate token counts
  ‚Üì
5. Trim to fit within maxTokens budget
  ‚Üì
6. Return enhanced context
  ‚Üì
agentService.getConversationHistory()
  ‚Üì
7. Format for model consumption
  - Add critical instructions
  - Add summaries as system message
  - Add pins as system message
  - Add older messages (condensed)
  - Add recent messages (full)
  ‚Üì
8. Return formatted history array
```

### Memory Context Example

**Input**: 100-message conversation

**MemoryManager Output**:
```typescript
{
  recentMessages: [
    { role: 'user', text: '...', importance: 0.8 },
    { role: 'assistant', text: '...', importance: 0.7 },
    // ... 8 more
  ],
  summaries: [
    { 
      summary: 'User discussed grandmother\'s medication schedule',
      message_ids: ['msg_1', 'msg_2', ...],
      created_at: '...'
    }
  ],
  semanticPins: [
    { 
      content: 'Patient allergic to penicillin',
      created_at: '...'
    }
  ],
  totalTokens: 2847
}
```

**agentService Output** (formatted for model):
```typescript
[
  { role: 'system', content: 'CRITICAL INSTRUCTION: ...' },
  { role: 'system', content: 'REFERENCE ONLY - Background: User discussed medication schedule' },
  { role: 'system', content: 'REFERENCE MATERIAL - Important: Patient allergic to penicillin' },
  { role: 'system', content: 'HISTORICAL REFERENCE: USER: ...\nASSISTANT: ...' },
  { role: 'system', content: '--- RECENT CONVERSATION ---' },
  { role: 'user', content: 'RECENT EXCHANGE: ...' },
  { role: 'assistant', content: 'RECENT EXCHANGE: ...' }
]
```

---

## Eldercare Context Integration

### EldercareContextService

**Purpose**: Dynamically inject relevant eldercare data into system prompt.

**Smart Context Selection**:
- Analyzes user query for keywords
- Checks model capabilities (token limits)
- Selects relevant patient/medication data
- Formats for model consumption

**Context Types**:
1. **Patient Information**: Demographics, conditions, caregiver
2. **Medications**: Current medications, schedules, side effects
3. **Vitals**: Recent weight, glucose readings
4. **Appointments**: Upcoming medical appointments

**Example Output**:
```
Patient Information:
- Name: Mary Smith
- Age: 82
- Conditions: Hypertension, Type 2 Diabetes
- Primary Caregiver: John Smith (Grandson)

Current Medications:
- Lisinopril 10mg - Once daily at 8am (for hypertension)
- Metformin 500mg - Twice daily with meals (for diabetes)
  Side effects to monitor: Nausea, diarrhea

Recent Vitals (Last 7 days):
- Weight: 145-148 lbs (stable)
- Blood Glucose: AM 95-110, PM 120-135 mg/dL (well controlled)

Upcoming Appointments:
- Dr. Johnson (Cardiologist) - Nov 5, 2024 at 10:00 AM
```

**Integration Point**: Called in `buildSystemPrompt()`.

---

## Best Practices

### 1. Always Handle Errors Gracefully

```typescript
// ‚úÖ GOOD: Graceful degradation
try {
  const persona = getPersonaPrompt(personaId)
} catch (error) {
  console.error('Error:', error)
  return ''  // Continue with empty prompt
}

// ‚ùå BAD: Throw and crash
const persona = db.prepare('...').get(personaId)  // Could throw
```

### 2. Merge Settings Correctly

```typescript
// ‚úÖ GOOD: Persona as base, request overrides
const merged = { ...personaSettings, ...requestSettings }

// ‚ùå BAD: Request as base (persona can't set defaults)
const merged = { ...requestSettings, ...personaSettings }
```

### 3. Validate Streaming Support

```typescript
// ‚úÖ GOOD: Runtime check
if (!('generateStream' in adapter)) {
  throw new Error('Streaming not supported')
}

// ‚ùå BAD: Assume support
for await (const chunk of adapter.generateStream(...)) {
  // Might crash if method doesn't exist
}
```

### 4. Use Tools Conditionally

```typescript
// ‚úÖ GOOD: Cloud models only
const tools = adapter.type === 'cloud' ? AVAILABLE_TOOLS : undefined

// ‚ùå BAD: Always use tools
const tools = AVAILABLE_TOOLS  // Breaks on Ollama
```

### 5. Add Markers for UI Features

```typescript
// ‚úÖ GOOD: UI can detect and show loading state
result.reply = '[SEARCHING_ONLINE]\n\n' + result.reply

// ‚ùå BAD: No indication to user
result.reply = result.reply  // User doesn't know search happened
```

### 6. Prevent Tool Recursion

```typescript
// ‚úÖ GOOD: Second generation without tools
const result = await adapter.generate({ 
  messages: newMessages, 
  settings, 
  fileIds 
  // No tools parameter
})

// ‚ùå BAD: Tools in second call (infinite loop risk)
const result = await adapter.generate({ 
  messages: newMessages, 
  settings, 
  fileIds,
  tools  // Could trigger more tool calls!
})
```

---

## Performance Considerations

### Token Budget Management

**Default**: 3000 tokens for conversation history

**Why 3000?**
- Most models support 4K-8K context windows
- Leaves room for system prompt (500-1000 tokens)
- Leaves room for response (1000-4000 tokens)
- Balances context richness vs response quality

**Calculation**:
```
Total Context = System Prompt + Conversation History + User Input + Response
4096 tokens   = 500 tokens    + 3000 tokens           + 100 tokens + 496 tokens
```

### Memory System Efficiency

**Without Hybrid Memory**:
- Include all 100 messages in context
- Exceeds token limit
- Model can't see older context

**With Hybrid Memory**:
- Summaries: 100 messages ‚Üí 200 tokens
- Pins: 5 important facts ‚Üí 50 tokens
- Recent: 10 messages ‚Üí 1500 tokens
- Total: ~1750 tokens (fits comfortably)

**Compression Ratio**: ~57x for older messages.

### Streaming Performance

**Advantages**:
- Faster time-to-first-token
- Better user experience (progressive display)
- Lower perceived latency

**Disadvantages**:
- Can't easily cancel mid-stream
- More complex error handling
- Harder to implement retries

---

## Security Considerations

### 1. Input Validation

**Current**: Validation happens at router level (Zod schemas).

**Improvement**: Add service-level validation for critical inputs.

### 2. Prompt Injection Protection

**Current Mitigations**:
- System messages clearly labeled as "REFERENCE"
- Critical instructions emphasize current query focus
- Conversation history marked as background

**Potential Risk**: User could craft input to override instructions.

**Future**: Implement prompt injection detection.

### 3. Tool Execution Safety

**Current**: Only trusted tools (Tavily search).

**Risk**: If adding user-defined tools, need sandboxing.

### 4. Error Message Sanitization

**Current**: Logs full errors to console.

**Production**: Should sanitize before returning to client.

---

## Testing Strategies

### Unit Tests

```typescript
describe('getPersonaPrompt', () => {
  it('should return prompt for valid persona', () => {
    const prompt = getPersonaPrompt('persona_test')
    expect(prompt).toBe('You are a test persona')
  })

  it('should return empty string for invalid persona', () => {
    const prompt = getPersonaPrompt('nonexistent')
    expect(prompt).toBe('')
  })

  it('should return empty string when no personaId', () => {
    const prompt = getPersonaPrompt()
    expect(prompt).toBe('')
  })
})
```

### Integration Tests

```typescript
describe('runAgent', () => {
  it('should generate response with persona', async () => {
    const result = await runAgent({
      modelName: 'gpt-4o-mini',
      input: 'Hello',
      personaId: 'persona_test',
      settings: { model: 'gpt-4o-mini' }
    })
    
    expect(result.reply).toBeDefined()
    expect(result.tokenUsage).toBeGreaterThan(0)
  })

  it('should handle tool calls', async () => {
    const result = await runAgent({
      modelName: 'gpt-4o-mini',
      input: 'What is the weather today?',
      settings: { model: 'gpt-4o-mini' }
    })
    
    expect(result.reply).toContain('[SEARCHING_ONLINE]')
  })
})
```

---

## Summary

The **Agent Service** is the orchestration heart of Kalito's AI system:

**Core Responsibilities**:
- Build intelligent, context-aware prompts
- Manage conversation history with hybrid memory
- Execute AI models (OpenAI, Ollama)
- Handle function calling (web search)
- Support streaming responses
- Integrate eldercare context dynamically

**Key Features**:
- **Hybrid Memory**: Summaries + Pins + Recent messages
- **Smart Context**: Token-aware, relevance-based
- **Function Calling**: Automatic tool execution
- **Eldercare Integration**: Dynamic patient/medication context
- **Persona Support**: Customizable AI personalities
- **Cross-Platform**: Works with cloud and local models

**Exports**:
- `runAgent()` - Non-streaming AI generation
- `runAgentStream()` - Streaming AI generation  
- `getAllSessions()` - Session list retrieval

**Dependencies**:
- `modelRegistry` - Model adapter selection
- `memoryManager` - Conversation context
- `eldercareContextService` - Patient data
- `tools` - Function calling
- Database - Persona settings, fallback history

**Production Status**: Fully implemented and production-ready with comprehensive error handling and fallback strategies.
